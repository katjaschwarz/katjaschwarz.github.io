<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> A Recipe for Generating 3D Worlds From a Single Image | Katja Schwarz </title> <meta name="author" content="Katja Schwarz"> <meta name="description" content="Academic webpage of Katja Schwarz. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https:/katjaschwarz.github.io/worlds/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous"> <link rel="stylesheet" href="../assets/css/project_page.css"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light-noframe navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> Katja Schwarz </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/">about</a> <a class="dropdown-item" href="/publications/">publications</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container"> <div style="text-align: center;"> <h1>A Recipe for Generating 3D Worlds From a Single Image</h1> <h3></h3> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <span style="font-size: 1.2em;"><a href="/">Katja Schwarz</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Denis Rozumny</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Samuel Rota Bulo</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Lorenzo Porzi</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Peter Kontschieder</a><sup>1</sup></span> <div> <span style="font-size: 1.2em;"><sup>1</sup>Meta Reality Labs</span> </div> <div style="margin-top: 0.5em;"> <span style="font-size: 1.4em;">ARXIV 2025</span> </div> <div style="margin-top: 0.5em; font-size: 1.7em;"> <a href="">[Paper]</a> </div> </li></ol> </div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/worlds/teaser-480.webp 480w,/assets/img/worlds/teaser-800.webp 800w,/assets/img/worlds/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/worlds/teaser.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Given a single input image, our pipeline generates a 360 degree world. The scene is parameterized by Gaussian Splats and can be explored on a VR headset within a cube with 2m side length. </div> <div class="section"> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="abstract"> <h2 class="text-left text-secondary">Abstract</h2> <p>We introduce a recipe for generating immersive 3D worlds from a single image by framing the task as an in-context learning problem for 2D inpainting models. This approach requires minimal training and uses existing generative models. Our process involves two steps: generating coherent panoramas using a pre-trained diffusion model and lifting these into 3D with a metric depth estimator. We then fill unobserved regions by conditioning the inpainting model on rendered point clouds, requiring minimal fine-tuning. Tested on both synthetic and real images, our method produces high-quality 3D environments suitable for VR display. By explicitly modeling the 3D structure of the generated environment from the start, our approach consistently outperforms state-of-the-art, video synthesis-based methods along multiple quantitative image quality metrics.</p> </div> </li></ol> </div> </div> <figure> <video src="/assets/img/worlds/wl04.mp4" class="img-fluid" width="100%" height="auto" autoplay="" loop="" muted=""></video> </figure> <figure> <video src="/assets/img/worlds/wl16.mp4" class="img-fluid" width="100%" height="auto" autoplay="" loop="" muted=""></video> </figure> <figure> <video src="/assets/img/worlds/wl17.mp4" class="img-fluid" width="100%" height="auto" autoplay="" loop="" muted=""></video> </figure> <div class="caption"> 3D scenes generated from a single input image. </div> <div class="section"> <h2 class="text-left text-secondary"> 3D Scene Synthesis as an In-Context Zero-Shot Learning Task </h2> <p> Our key insight is that the task of generating a 3D environment from a single image, which is inherently complex and ambiguous, can be decomposed into a series of more manageable sub-problems, each of which can be addressed with existing techniques. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/worlds/method-480.webp 480w,/assets/img/worlds/method-800.webp 800w,/assets/img/worlds/method-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/worlds/method.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p> We address 2D panorama synthesis as an in-context zero-shot learning task for existing inpainting models. By leveraging a vision-language model to generate prompts, our method can produce high-fidelity panoramic images without the need for additional training. Specifically, the vision-language model generates a non-specific prompt for scene extension, ensuring that key features or properties, e.g. a men holding tulips, are not duplicated. Moreover, we create separate prompts for the upper (sky) and lower (ground) sections of the panorama. To anchor sky and ground synthesis, the input image is duplicated to the backside of the panorama. The progressive synthesis process begins with the sky and ground, maximizing the global context and ensuring a coherent panorama. Then, the backside anchor is removed and the remaining regions of the panorama are generated by rendering and outpainting perspective images. In the second stage, the generated panorama is lifted into an approximately metric three-dimensional space. We first apply monocular, metric depth estimation on rendered images. This works sufficiently well for images rendered from the panorama, but usually leaves empty spots in previously occluded areas or at large depth discontinuities emerging when the camera views are shifted, i.e., underwent a translation. We identify this as another inpainting task, and demonstrate that the inpainting model can quickly adapt to this setting, when fine-tuned with appropriate masks derived from the rendered point clouds. Lastly, we use the lifted point cloud and the generated images to reconstruct a 3D scene, parameterized by Gaussian splats. The scene is viewable and navigable within a 2-meter cube on a VR headset. </p> </div> <div class="section"> <h2 class="text-left text-secondary"> Technical Contributions </h2> <ul> <li>We decompose 3D scene synthesis into two easier subproblems: panorama synthesis, and point cloud-conditional inpainting, enabling the generation of 360 degree navigable environments from a single input image.</li> <li>We propose a novel approach to panorama generation inspired by visual in-context learning, leading to more consistent sky and ground synthesis while enhancing overall image quality.</li> <li>For point cloud-conditioned inpainting, we propose a simple, yet efficient forward-backward warping strategy for fine-tuning a ControlNet with minimal training effort.</li> <li>We augment Gaussian Splatting (3DGS) with a distortion correction mechanism to account for minor remaining inconsistencies between generated multi-view images, leading to overall sharper and more detailed results.</li> </ul> </div> <div class="section"> <h2 class="text-left text-secondary"> Scene Synthesis From a Single Image </h2> <p> While existing approaches can generate good videos from a single input image, these methods are typically unable to produce a fully immersive scene, notably struggling with outpainting towards the opposite direction of the initial view. Further, the generated videos often lack consistency, resulting in artifacts in the reconstructed 3D scenes. </p> <h3>Baseline Comparison</h3> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>Reference Image</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/worlds/01-480.webp 480w,/assets/img/worlds/01-800.webp 800w,/assets/img/worlds/01-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/worlds/01.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>Ours</h4> <figure> <video src="/assets/img/worlds/Ours_01.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>DimensionX</h4> <figure> <video src="/assets/img/worlds/dimX_01.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>WonderJourney</h4> <figure> <video src="/assets/img/worlds/WJ_01.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/worlds/02-480.webp 480w,/assets/img/worlds/02-800.webp 800w,/assets/img/worlds/02-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/worlds/02.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/Ours_02.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/dimX_02.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/WJ_02.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/worlds/03-480.webp 480w,/assets/img/worlds/03-800.webp 800w,/assets/img/worlds/03-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/worlds/03.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/Ours_03.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/dimX_03.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/worlds/WJ_03.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <h3>More Results</h3> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/wl20.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/wl21.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/wl26.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/wl02.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/wl29.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Scene Synthesis From Text </h2> <p> Our pipeline natively extends to text-to-scene synthesis by first generating an image from the given text prompt and subsequently running our pipeline. </p> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/rainbow.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/noodle.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/worlds/cotton_kittens.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Citation </h2> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <pre style="background-color:#EAEAEA">
<code>
  @InProceedings{Schwarz2025worlds,
    author = {Schwarz, Katja and Rozumny, Denis and Rota Bulo, Samuel and Porzi, Lorenzo and Kontschieder, Peter},
    title = {A Recipe for Generating 3D Worlds From a Single Image},
    booktitle = {arXiv.org (ARXIV)},
    year = {2025}
  }</code>
</pre> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Katja Schwarz. Created using the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script src="https://cdn.jsdelivr.net/npm/jquery@/dist/jquery.min.js" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@/js/mdb.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@/dist/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script> </body> </html>