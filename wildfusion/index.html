<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> WildFusion | Katja Schwarz </title> <meta name="author" content="Katja Schwarz"> <meta name="description" content="Academic webpage of Katja Schwarz. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https:/katjaschwarz.github.io/wildfusion/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous"> <link rel="stylesheet" href="../assets/css/project_page.css"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light-noframe navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> Katja Schwarz </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/">about</a> <a class="dropdown-item" href="/publications/">publications</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container"> <div style="text-align: center;"> <h1>WildFusion</h1> <h3>Learning 3D-Aware Latent Diffusion Models in View Space</h3> <div class="bibentry"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <span style="font-size: 1.2em;"><a href="/">Katja Schwarz</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="https://seung-kim.github.io/seungkim/" rel="external nofollow noopener" target="_blank">Seung Wook Kim</a><sup>2,3,4</sup>,</span> <span style="font-size: 1.2em;"><a href="https://www.cs.toronto.edu/~jungao/" rel="external nofollow noopener" target="_blank">Jun Gao</a><sup>2,3,4</sup>,</span> <span style="font-size: 1.2em;"><a href="https://www.cs.utoronto.ca/~fidler/" rel="external nofollow noopener" target="_blank">Sanja Fidler</a><sup>2,3,4</sup>,</span> <span style="font-size: 1.2em;"><a href="http://www.cvlibs.net/" rel="external nofollow noopener" target="_blank">Andreas Geiger</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="https://karstenkreis.github.io/" rel="external nofollow noopener" target="_blank">Karsten Kreis</a><sup>2</sup></span> <div> <span style="font-size: 1.2em;"><sup>1</sup>University of Tübingen,</span> <span style="font-size: 1.2em;"><sup>2</sup>NVIDIA,</span> <span style="font-size: 1.2em;"><sup>3</sup>Vector Institute,</span> <span style="font-size: 1.2em;"><sup>4</sup>University of Toronto</span> </div> <div style="margin-top: 0.5em;"> <span style="font-size: 1.4em;">ICLR 2024</span> </div> <div style="margin-top: 0.5em; font-size: 1.7em;"> <a href="https://arxiv.org/pdf/2311.13570.pdf" rel="external nofollow noopener" target="_blank">[Paper]</a> </div> </li></ol> </div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wildfusion/teaser-480.webp 480w,/assets/img/wildfusion/teaser-800.webp 800w,/assets/img/wildfusion/teaser-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/wildfusion/teaser.jpg" class="img-fluid" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> WildFusion is a two-stage approach to 3D-aware image synthesis, trained only on unposed single view images. Left: Input images, novel views and geometry from our first-stage autoencoder. Right: Novel samples and geometry from our second-stage latent diffusion model and 3DGP for the ImageNet classes "macaw" (top), "king penguin" (middle), and "kimono" (bottom). </div> <div class="section"> <div class="bibentry"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="abstract"> <h2 class="text-left text-secondary">Abstract</h2> <p>Modern learning-based approaches to 3D-aware image synthesis achieve high photorealism and 3D-consistent viewpoint changes for the generated images. Existing approaches represent instances in a shared canonical space. However, for in-the-wild datasets a shared canonical system can be difficult to define or might not even exist. In this work, we instead model instances in <em>view space</em>, alleviating the need for posed images and learned camera distributions. We find that in this setting, existing GAN-based methods are prone to generating flat geometry and struggle with distribution coverage. We hence propose <em>WildFusion</em>, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs). We first train an autoencoder that infers a compressed latent representation, which additionally captures the images’ underlying 3D structure and enables not only reconstruction but also novel view synthesis. To learn a faithful 3D representation, we leverage cues from monocular depth prediction. Then, we train a diffusion model in the 3D-aware latent space, thereby enabling synthesis of high-quality 3D-consistent image samples, outperforming recent state-of-the-art GAN-based methods. Importantly, our 3D-aware LDM is trained without any direct supervision from multiview images or 3D geometry and does not require posed images or learned pose or camera distributions. It directly learns a 3D representation without relying on canonical camera coordinates. This opens up promising research avenues for scalable 3D-aware image synthesis and 3D content creation from in-the-wild image data. </p> </div> </li></ol> </div> </div> <figure> <video src="/assets/img/wildfusion/teaser3.mp4" class="img-fluid" width="100%" height="auto" autoplay="" loop="" muted=""></video> </figure> <div class="caption"> Novel samples from ImageNet classes generated by WildFusion. </div> <div class="section"> <h2 class="text-left text-secondary"> 3D-Aware Image Synthesis with Latent Diffusion Models in View Space </h2> <p> While existing 3D-aware generative models achieve high photorealism and 3D-consistent viewpoint control, the vast majority of approaches only consider single-class and aligned data like human faces or cat faces. We identify two main causes for this: </p> <ul> <li>Existing approaches assume a shared canonical coordinate system to represent 3D objects. As a consequence, they require either poses from an off-the-shelf pose estimator or assume, and sometimes learn to refine, a given pose distribution. In contrast, in-the-wild images typically have no clearly defined canonical camera system and camera poses or pose distributions are not available or very challenging to obtain. </li> <li>Most existing 3D-aware generative models are generative adversarial networks (GANs). We find that for complex datasets with challenging pose distributions GANs struggle with distribution coverage and suffer from mode collapse. </li> </ul> We hence propose <em>WildFusion</em>, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs) that addresses these limitations. We train our approach in view space which removes the need for camera poses and a priori camera pose distributions, unlocking 3D-aware image synthesis on unaligned, diverse datasets. To ensure distribution coverage on more diverse datasets, we build our approach upon latent diffusion models (LDMs) instead of GANs. <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wildfusion/system-480.webp 480w,/assets/img/wildfusion/system-800.webp 800w,/assets/img/wildfusion/system-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/wildfusion/system.png" class="img-fluid" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p> Our 3D-aware LDM, called WildFusion, follows LDMs’ two-stage approach: First, we train a powerful 3D-aware autoencoder from large collections of unposed images without multiview supervision that simultaneously performs both compression and enables novel-view synthesis. The autoencoder is trained with pixel-space reconstruction losses on the input views and uses adversarial training to supervise novel views. Note that by using adversarial supervision for the novel views, our autoencoder is trained for novel-view synthesis without the need for multiview supervision. Adding monocular depth cues helps the model learn a faithful 3D representation and further improves novel-view synthesis. In the second stage, we train a diffusion model in the compressed and 3D-aware latent space, which enables us to synthesize novel samples and turns the novel-view synthesis system, i.e., our autoencoder, into a 3D-aware generative model. </p> </div> <div class="section"> <h2 class="text-left text-secondary"> Technical Contributions </h2> <ul> <li>We remove the need for posed images and a priori camera pose distributions for 3D-aware image synthesis by modeling instances in <em>view space</em> instead of canonical space.</li> <li>We learn a powerful <em>3D-aware autoencoder</em> from unposed images without multiview supervision that simultaneously performs compression, while inferring a 3D representation suitable for novel-view synthesis.</li> <li>We show that our novel <em>3D-aware LDM</em>, WildFusion, enables high-quality 3D-aware image synthesis with reasonable geometry and strong distribution coverage, achieving state-of-the-art performance in the unposed image training setting, which corresponds to training on in-the-wild image data.</li> </ul> </div> <div class="section"> <h2 class="text-left text-secondary"> Baseline Comparison </h2> <p> We find that existing GAN-based approaches struggle with very low sample diversity (mode collapse) on multi-modal datasets with complex camera distributions like ImageNet. The results below show samples from WildFusion and <a href="https://snap-research.github.io/3dgp/" rel="external nofollow noopener" target="_blank">3DGP</a>, the strongest baseline, where each row corresponds to samples of one class. While 3DGP collapses and produces almost identical samples within classes, WildFusion produces diverse, high-quality samples because it builds upon Latent Diffusion Models. </p> <div class="row"> <div class="col-md-6 col-sm-6 col-xs-6 gallery"> <h4>WildFusion</h4> <figure> <video src="/assets/img/wildfusion/blcomp_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-6 col-sm-6 col-xs-6 gallery"> <h4>3DGP</h4> <figure> <video src="/assets/img/wildfusion/blcomp_3dgp.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Autoencoder for Compression and Novel View Synthesis </h2> <p> Our 3D-aware autoencoder performs both compression and enables novel-view synthesis. Notably, it is trained from large collections of unposed images without any direct multiview supervision. The learned compressed 3D-aware latent space can then be used to train a latent diffusion model. In addition, we can leverage our autoencoder to more efficiently perform novel view synthesis for a single given image than common GAN-based methods relying on GAN-inversion. We show pairs of input images and synthesized novel views from our autoencoder below. </p> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <h4>ImageNet</h4> <figure> <video src="/assets/img/wildfusion/ae1.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/ae2.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <h4>SDIP Dog/Horse/Elephant</h4> <figure> <video src="/assets/img/wildfusion/ae_sdip.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Generated Samples </h2> <p> We train a latent diffusion model on the compressed 3D-aware latent space of the 3D-aware autoencoder. Our 3D-aware LDM enables high-quality 3D-aware image synthesis with reasonable geometry and strong distribution coverage / high sample diversity. </p> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <h4>ImageNet</h4> <figure> <video src="/assets/img/wildfusion/gen1.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/gen2.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/gen3.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/gen4.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <h4>SDIP Dog/Horse/Elephant</h4> <figure> <video src="/assets/img/wildfusion/gen_sdip.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Interpolation </h2> <p> Using WildFusion, we can interpolate in a semantically meaningful way between two given single images while simultaneously allowing to change the viewpoint. Note that the geometry also changes accordingly. Specifically, we encode two images into latent space, further encode into the diffusion model’s Gaussian prior space (inverse DDIM), interpolate the resulting encodings, and generate the corresponding 3D images along the interpolation path. </p> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/interp.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Generative Resampling with Different Noise Levels </h2> <p> We can further use WildFusion to perform 3D-aware generative image resampling. Given an image, we forward diffuse its latent encoding for varying numbers of steps and re-generate from the partially noised encodings. Depending on how far we diffuse, we control how strongly the sample adheres to the input image. For the samples below, we gradually increase the number of diffusion steps from left to right. </p> <div class="row"> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/wildfusion/refinement.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Citation </h2> <div class="bibentry"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <pre style="background-color:#EAEAEA">
<code>
  @InProceedings{Schwarz2024ICLR,
    author = {Schwarz, Katja and Wook Kim, Seung and Gao, Jun and Fidler, Sanja and Geiger, Andreas and Kreis, Karsten},
    title = {WildFusion: Learning 3D-Aware Latent Diffusion Models in View Space},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year = {2024}
  }</code>
</pre> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Katja Schwarz. Created using the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>