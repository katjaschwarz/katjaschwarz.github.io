<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Generative Gaussian Splatting | Katja Schwarz </title> <meta name="author" content="Katja Schwarz"> <meta name="description" content="Academic webpage of Katja Schwarz. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%B8&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https:/katjaschwarz.github.io/ggs/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous"> <link rel="stylesheet" href="../assets/css/project_page.css"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light-noframe navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item dropdown"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"> Katja Schwarz </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/">about</a> <a class="dropdown-item" href="/publications/">publications</a> </div> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container"> <div style="text-align: center;"> <h1>Generative Gaussian Splatting</h1> <h3>Generating 3D Scenes with Video Diffusion Priors</h3> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <span style="font-size: 1.2em;"><a href="/">Katja Schwarz</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Norman Müller</a><sup>1</sup>,</span> <span style="font-size: 1.2em;"><a href="/">Peter Kontschieder</a><sup>1</sup></span> <div> <span style="font-size: 1.2em;"><sup>1</sup>Meta Reality Labs Zurich, Switzerland</span> </div> <div style="margin-top: 0.5em;"> <span style="font-size: 1.4em;">ICCV 2025</span> </div> <div style="margin-top: 0.5em; font-size: 1.7em;"> <a href="https://arxiv.org/abs/2503.13272" rel="external nofollow noopener" target="_blank">[Paper]</a> </div> </li></ol> </div> </div> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/teaser-480.webp 480w,/assets/img/ggs/teaser-800.webp 800w,/assets/img/ggs/teaser-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/teaser.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> Given one or more input images, GGS leverages a video diffusion prior to directly generate a 3D radiance field parameterized via 3D Gaussian primitives. GGS first generates a feature field with a pose-conditional diffusion model and subsequently decodes the feature splats, yielding an explicit 3D representation of the generated scene. </div> <div class="section"> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="abstract"> <h2 class="text-left text-secondary">Abstract</h2> <p>Synthesizing consistent and photorealistic 3D scenes is an open problem in computer vision. Video diffusion models generate impressive videos but cannot directly synthesize 3D representations, i.e, lack 3D consistency in the generated sequences. In addition, directly training generative 3D models is challenging due to a lack of 3D training data at scale. In this work, we present Generative Gaussian Splatting (GGS) - a novel approach that integrates a 3D representation with a pre-trained latent video diffusion model. Specifically, our model synthesizes a feature field parameterized via 3D Gaussian primitives. The feature field is then either rendered to feature maps and decoded into multi-view images, or directly upsampled into a 3D radiance field. We evaluate our approach on two common benchmark datasets for scene synthesis, RealEstate10K and ScanNet++, and find that our proposed GGS model significantly improves both the 3D consistency of the generated multi-view images, and the quality of the generated 3D scenes over all relevant baselines. Compared to a similar model without 3D representation, GGS improves FID on the generated 3D scenes by 20% on both RealEstate10K and ScanNet++.</p> </div> </li></ol> </div> </div> <figure> <video src="/assets/img/ggs/ggs_teaser.mp4" class="img-fluid" width="100%" height="auto" autoplay="" loop="" muted=""></video> </figure> <div class="caption"> 3D scene generated with GGS from a single input image. </div> <div class="section"> <h2 class="text-left text-secondary"> Generating 3D Scenes with Video Diffusion Priors </h2> <p> While existing pose-conditional video diffusion models models achieve high photorealism, they often lack 3D consistency and cannot directly synthesize 3D representations​. Instead, we propose to directly integrate an explicit 3D representation with a pre-trained latent video diffusion model. Our approach, GGS, improves 3D consistency in the generated images and naturally allows training with additional depth supervision where available. We further design a custom decoder that directly predicts the decoded 3D representation of the scene from the generated feature maps. </p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/system-480.webp 480w,/assets/img/ggs/system-800.webp 800w,/assets/img/ggs/system-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/system.jpg" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p> Our approach, GGS, directly synthesizes a 3D representation, which is parameterized by a set of Gaussian splats \(\{\mathbf{g}^m\}\), from a set of posed input images. Specifically, during training we consider a set of posed images \(\{\mathbf{I}^m\}\) with associated camera poses \(\{\mathbf{p}^m\}\) and corresponding Pluecker embeddings \(\{\mathbf{P}^m\}\). The images are first encoded into a latent representation \(\{\mathbf{z}_0^m\}\), which is then partitioned into \(K\) reference images and \(L\) target images. We introduce noise only to the latents of the target images \(\{\mathbf{z}_{tgt,0}^l\}_{l=1}^L\), while leaving the reference images noise-free. To ensure compatibility with the pretrained image-to-video diffusion model, we duplicate the reference latents across the channel dimension and concatenate zeros for the target latents. The resulting latents, along with the noise level \(\sigma_t\) and Pluecker embeddings, are fed into a U-Net architecture that produces intermediate per-latent feature maps. These feature maps are subsequently processed by an epipolar transformer \(\mathcal{T}_{epi}\) to predict the parameters of the Gaussian feature splats \(\{\mathbf{g}^m\}\). We render both feature maps \(\{\mathbf{f}^m\}\) and low-resolution images \(\{\mathbf{I}_{LR}^m\}\) for the input views, as well as low-resolution images for \(J\) novel views \(\{\mathbf{I}_{nv,LR}^j\}_{j=1}^J\) to regularize the 3D representation. Finally, the rendered feature maps are decoded into a weighted combination of sample noise \(\mathbf{\xi}^m\) and input latent to predict the noise-free latents \(\{\hat{\mathbf{z}}_0^m\}\). </p> </div> <div class="section"> <h2 class="text-left text-secondary"> Technical Contributions </h2> <ul> <li>We propose an approach that <em>directly integrates an explicit 3D representation with a pre-trained latent video diffusion backbone</em>, thereby improving 3D consistency of the generated image sequences and allowing for training with additional depth supervision where available.</li> <li>We design a <em>custom decoder that directly predicts the decoded 3D representation</em> of the scene from the generated feature maps.</li> <li>We train a <em>conditional variant of our model that auto-regressively generates full scenes</em> from an arbitrary number of input views.</li> </ul> </div> <div class="section"> <h2 class="text-left text-secondary"> Sequence Synthesis From a Single Image </h2> <p> Existing pose-conditional diffusion models without 3D representation often struggle to generate 3D consistent sequences. While <a href="https://hehao13.github.io/projects-CameraCtrl/" rel="external nofollow noopener" target="_blank">CameraCtrl</a> can generate reasonable sequences, it might not accurately follow the given camera trajectory. E.g., compare the position of the chair in the lower part of the image at the end of the sequence. <a href="https://drexubery.github.io/ViewCrafter/" rel="external nofollow noopener" target="_blank">ViewCrafter</a> follows the trajectory more closely but often fails to preserve the appearance of the content. It also relies on correct depth estimates and wrong depth prediction can result in artifacts in the generated sequences, see bottom row. </p> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>Reference Image</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/1_ref/10ad4fc499c48b38-480.webp 480w,/assets/img/ggs/results/1_ref/10ad4fc499c48b38-800.webp 800w,/assets/img/ggs/results/1_ref/10ad4fc499c48b38-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/1_ref/10ad4fc499c48b38.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>GGS</h4> <figure> <video src="/assets/img/ggs/results/1_ref/10ad4fc499c48b38_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>CameraCtrl</h4> <figure> <video src="/assets/img/ggs/results/1_ref/10ad4fc499c48b38_cameractrl.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>ViewCrafter</h4> <figure> <video src="/assets/img/ggs/results/1_ref/10ad4fc499c48b38_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/1_ref/179ff8424ec7ad13-480.webp 480w,/assets/img/ggs/results/1_ref/179ff8424ec7ad13-800.webp 800w,/assets/img/ggs/results/1_ref/179ff8424ec7ad13-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/1_ref/179ff8424ec7ad13.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/179ff8424ec7ad13_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/179ff8424ec7ad13_cameractrl.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/179ff8424ec7ad13_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/1_ref/75d29d69b8-480.webp 480w,/assets/img/ggs/results/1_ref/75d29d69b8-800.webp 800w,/assets/img/ggs/results/1_ref/75d29d69b8-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/1_ref/75d29d69b8.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/75d29d69b8_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/75d29d69b8_cameractrl.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/75d29d69b8_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> 3D Scene Synthesis From a Single Image </h2> <p> Below, we show 3D Gaussian splats generated from a single image using GGS. We also provide the reference images and generated feature splats. </p> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/ggs/results/1_ref/scene_synthesis_single.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> View Extrapolation From Two Images </h2> <p> <a href="https://geometric-rl.mpi-inf.mpg.de/latentsplat/" rel="external nofollow noopener" target="_blank">LatentSplat</a> performs well for small camera baselines but its GAN-based generative decoder struggles with large viewpoint extrapolations. Instead, our diffusion model can generate reasonable and consistent images also for larger extrapolations. Similar to the single image setting, ViewCrafter performs overall well wrt. the camera trajectory but can alter the generated content between frames. </p> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>Reference Image</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/2_ref/3ad4793daf6adc19-480.webp 480w,/assets/img/ggs/results/2_ref/3ad4793daf6adc19-800.webp 800w,/assets/img/ggs/results/2_ref/3ad4793daf6adc19-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/2_ref/3ad4793daf6adc19.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>GGS</h4> <figure> <video src="/assets/img/ggs/results/2_ref/3ad4793daf6adc19_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>CameraCtrl</h4> <figure> <video src="/assets/img/ggs/results/2_ref/3ad4793daf6adc19_latentsplat.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <h4>ViewCrafter</h4> <figure> <video src="/assets/img/ggs/results/2_ref/3ad4793daf6adc19_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/2_ref/17a39d87a22ac1ec-480.webp 480w,/assets/img/ggs/results/2_ref/17a39d87a22ac1ec-800.webp 800w,/assets/img/ggs/results/2_ref/17a39d87a22ac1ec-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/2_ref/17a39d87a22ac1ec.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/17a39d87a22ac1ec_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/17a39d87a22ac1ec_latentsplat.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/17a39d87a22ac1ec_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="row"> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/ggs/results/2_ref/e8e81396b6-480.webp 480w,/assets/img/ggs/results/2_ref/e8e81396b6-800.webp 800w,/assets/img/ggs/results/2_ref/e8e81396b6-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/ggs/results/2_ref/e8e81396b6.png" class="img-fluid" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/e8e81396b6_ours.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/e8e81396b6_latentsplat.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> <div class="col-md-3 col-sm-3 col-xs-3 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/e8e81396b6_vc.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> 3D Scene Synthesis From Two Images </h2> <p> Below, we compare the generated 3D scenes from two images for GGS and the strongest baseline ViewCrafter. Results were obtained by running Splatfacto as an off-the-shelf 3D reconstruction algorithm. As GGS directly generates a 3D representation with its 3D decoder branch, we can leverage the generated 3D splats as initialization for the reconstruction. </p> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/scene_synthesis_two.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Autoregressive Scene Synthesis From Five Images </h2> <p> Lastly, we show results for the conditional vairant of our GGS model. We autoregressively generate larger scenes using only 5 reference images. </p> <div class="col-md-12 col-sm-12 col-xs-12 gallery"> <figure> <video src="/assets/img/ggs/results/2_ref/scene_synthesis_autoregressive.mp4" class="img-fluid" width="100%" height="auto" autoplay="" controls="" loop="" muted=""></video> </figure> </div> </div> <div class="section"> <h2 class="text-left text-secondary"> Citation </h2> <div class="bibentry"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <pre style="background-color:#EAEAEA">
<code>
  @InProceedings{Schwarz2025ggs,
    author = {Schwarz, Katja and Müller, Norman and Kontschieder, Peter},
    title = {Generative Gaussian Splatting: Generating 3D Scenes with Video Diffusion Priors},
    booktitle = {Proc. of the IEEE International Conf. on Computer Vision (ICCV) (ICCV)},
    year = {2025}
  }</code>
</pre> </li></ol> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Katja Schwarz. Created using the <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script src="https://cdn.jsdelivr.net/npm/jquery@/dist/jquery.min.js" integrity="" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@/js/mdb.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@/dist/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@/dist/medium-zoom.min.js" integrity="" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script> </body> </html>