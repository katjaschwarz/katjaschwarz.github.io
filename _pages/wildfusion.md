---
layout: project_page
permalink: /wildfusion/
title: WildFusion
subtitle: Learning 3D-Aware Latent Diffusion Models in View Space
nav: true
nav_order: 2
horizontal: false

affiliation_names: [University of Tübingen, NVIDIA, Vector Institute, University of Toronto]
affiliations:
  - author: Schwarz
    index: [1]
  - author: Wook Kim
    index: [2, 3, 4]
  - author: Gao
    index: [2, 3, 4]
  - author: Fidler
    index: [2, 3, 4]
  - author: Geiger
    index: [1]
  - author: Kreis
    index: [2]
bibkey: Schwarz2024ICLR
---

{% include figure.liquid path="assets/img/wildfusion/teaser.jpg" class="img-fluid" %}

<div class="caption">
    WildFusion is a two-stage approach to 3D-aware image synthesis, trained only on unposed single view images. Left: Input images, novel views and geometry from our first-stage autoencoder. Right: Novel 
    samples and geometry from our second-stage latent diffusion model and 3DGP for the ImageNet classes "macaw" (top), "king penguin" (middle), and "kimono" (bottom).
</div>

<div class="section">
<div class="bibentry">
    {% bibliography --query @*[key={{page.bibkey}}]* -T bib_abstract %}
</div>
</div>

{% include video.liquid path="assets/img/wildfusion/teaser3.mp4" class="img-fluid" controls=false autoplay=true
loop=true muted=true preload=true width="100%" %}

<div class="caption">
    Novel samples from ImageNet classes generated by WildFusion.
</div>

<div class="section">
<h2 class="text-left text-secondary">
    3D-Aware Image Synthesis with Latent Diffusion Models in View Space
</h2>
<p>
    While existing 3D-aware generative models achieve high photorealism and 3D-consistent viewpoint control, the vast majority of approaches only consider single-class and aligned data like human faces or cat faces. We identify two main causes for this: 
    <ul>
        <li>Existing approaches assume a shared canonical coordinate system to represent 3D objects. As a consequence,
        they require either poses from an off-the-shelf pose estimator or assume, and sometimes learn to refine, a given pose
        distribution. In contrast, in-the-wild images typically have no clearly defined canonical camera
        system and camera poses or pose distributions are not available or very challenging to obtain.
        </li>
        <li>Most existing 3D-aware generative models are generative adversarial networks (GANs). We find that for complex datasets with challenging pose distributions GANs struggle with distribution coverage and suffer from mode collapse.
        </li>
    </ul>
    We hence propose <em>WildFusion</em>, a new approach to 3D-aware image synthesis based on latent diffusion models (LDMs) that addresses these limitations.
    We train our approach in view space which removes the need for camera poses and a priori camera pose distributions, unlocking 3D-aware image synthesis on unaligned, diverse datasets. To ensure distribution coverage on more diverse datasets, we build our approach upon latent diffusion models (LDMs) instead of GANs.
</p>
{% include figure.liquid path="assets/img/wildfusion/system.png" class="img-fluid" %}
<p>
Our 3D-aware LDM, called WildFusion, follows LDMs’ two-stage approach: First, we train a
powerful 3D-aware autoencoder from large collections of unposed images without multiview
supervision that simultaneously performs both compression and enables novel-view synthesis. The
autoencoder is trained with pixel-space reconstruction losses on the input views and uses adversarial training to supervise novel views. Note that by using adversarial supervision for the novel views, our autoencoder is trained for novel-view synthesis without the need for multiview supervision. Adding monocular depth cues helps the model learn a faithful 3D representation and further improves novel-view synthesis. In the second stage, we train a diffusion model in the compressed and 3D-aware latent space, which enables us to synthesize novel samples and turns the novel-view synthesis system, i.e., our autoencoder, into a 3D-aware generative model.
</p>
</div>

<div class="section">
<h2 class="text-left text-secondary">
    Technical Contributions
</h2>
<ul>
    <li>We remove the need for posed images and a priori camera pose distributions for 3D-aware image synthesis by modeling instances in <em>view space</em> instead of canonical space.</li>
    <li>We learn a powerful <em>3D-aware autoencoder</em> from unposed images without multiview supervision that simultaneously performs compression, while inferring a 3D representation suitable for novel-view synthesis.</li>
    <li>We show that our novel <em>3D-aware LDM</em>, WildFusion, enables high-quality 3D-aware image synthesis with reasonable geometry and strong distribution coverage, achieving state-of-the-art performance in the unposed image training setting, which corresponds to training on in-the-wild image data.</li>
</ul>
</div>

<div class="section">
<h2 class="text-left text-secondary">
    Baseline Comparison
</h2>   
<p>
  We find that existing GAN-based approaches struggle with very low sample diversity (mode collapse) on multi-modal datasets with complex camera distributions like ImageNet.
  The results below show samples from WildFusion and <a href="https://snap-research.github.io/3dgp/">3DGP</a>, the strongest baseline, where each row corresponds to samples of one class.
  While 3DGP collapses and produces almost identical samples within classes, WildFusion produces diverse, high-quality samples because it builds upon Latent Diffusion Models.
</p>
<div class="row">
    <div class="col-md-6 col-sm-6 col-xs-6 gallery">
        <h4>WildFusion</h4>
        {% include video.liquid path="assets/img/wildfusion/blcomp_ours.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
    <div class="col-md-6 col-sm-6 col-xs-6 gallery">
        <h4>3DGP</h4>
        {% include video.liquid path="assets/img/wildfusion/blcomp_3dgp.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
</div>
</div>

<div class="section">
<h2 class="text-left text-secondary">
Autoencoder for Compression and Novel View Synthesis
</h2>
<p>
Our 3D-aware autoencoder performs both compression and enables novel-view synthesis. Notably, it is trained from large collections of unposed images without any direct multiview supervision.
The learned compressed 3D-aware latent space can then be used to train a latent diffusion model. In addition, we can leverage our autoencoder to more efficiently perform novel view synthesis for a single given image than common GAN-based methods relying on GAN-inversion.
We show pairs of input images and synthesized novel views from our autoencoder below.
</p>
<div class="row">
<div class="col-md-12 col-sm-12 col-xs-12 gallery">
  <h4>ImageNet</h4>
        {% include video.liquid path="assets/img/wildfusion/ae1.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
</div>
<div class="col-md-12 col-sm-12 col-xs-12 gallery">
        {% include video.liquid path="assets/img/wildfusion/ae2.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
</div>
<div class="col-md-12 col-sm-12 col-xs-12 gallery">
  <h4>SDIP Dog/Horse/Elephant</h4>
        {% include video.liquid path="assets/img/wildfusion/ae_sdip.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
</div>
</div>
</div>

<div class="section">
<h2 class="text-left text-secondary">
Generated Samples
</h2>
<p>
We train a latent diffusion model on the compressed 3D-aware latent space of the 3D-aware autoencoder.
Our 3D-aware LDM enables high-quality 3D-aware image synthesis with reasonable geometry and strong distribution coverage / high sample diversity. 
  </p>
<div class="row">
    <div class="col-md-12 col-sm-12 col-xs-12 gallery">
      <h4>ImageNet</h4>
        {% include video.liquid path="assets/img/wildfusion/gen1.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
    <div class="col-md-12 col-sm-12 col-xs-12 gallery">
        {% include video.liquid path="assets/img/wildfusion/gen2.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
    <div class="col-md-12 col-sm-12 col-xs-12 gallery">
        {% include video.liquid path="assets/img/wildfusion/gen3.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
    <div class="col-md-12 col-sm-12 col-xs-12 gallery">
             {% include video.liquid path="assets/img/wildfusion/gen4.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
    <div class="col-md-12 col-sm-12 col-xs-12 gallery">
      <h4>SDIP Dog/Horse/Elephant</h4>
              {% include video.liquid path="assets/img/wildfusion/gen_sdip.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
    </div>
</div>
</div>

<div class="section">
<h2 class="text-left text-secondary">
Interpolation
</h2>
<p>
Using WildFusion, we can interpolate in a semantically meaningful way between two given single images while simultaneously allowing to change the viewpoint. Note that the geometry also changes accordingly. Specifically, we encode two images into latent space, further encode into the diffusion model’s Gaussian prior space (inverse DDIM), interpolate the resulting encodings, and generate the corresponding 3D images along the interpolation path. 
</p>
<div class="row">
<div class="col-md-12 col-sm-12 col-xs-12 gallery">
    {% include video.liquid path="assets/img/wildfusion/interp.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
</div>
</div>
</div>

<div class="section">
<h2 class="text-left text-secondary">
Generative Resampling with Different Noise Levels
</h2>
<p>
We can further use WildFusion to perform 3D-aware generative image resampling. Given an image, we forward diffuse its latent encoding for varying numbers of steps and re-generate from the partially noised encodings. Depending on how far we diffuse, we control how strongly the sample adheres to the input image. For the samples below, we gradually increase the number of diffusion steps from left to right.
</p>
<div class="row">
<div class="col-md-12 col-sm-12 col-xs-12 gallery">
    {% include video.liquid path="assets/img/wildfusion/refinement.mp4" class="img-fluid" controls=true autoplay=true loop=true muted=true preload=true width="100%" %}
</div>
</div>
</div>

<div class="section">
<h2 class="text-left text-secondary">
Citation
</h2>
<div class="bibentry">
    {% bibliography --query @*[key={{page.bibkey}}]* -T bib_citationbox %}
</div>
</div>
